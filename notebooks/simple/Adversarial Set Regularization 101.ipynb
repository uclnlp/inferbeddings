{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def read_triples(path):\n",
    "    triples = []\n",
    "    with open(path, 'rt') as f:\n",
    "        for line in f.readlines():\n",
    "            s, p, o = line.split()\n",
    "            triples += [(s.strip(), p.strip(), o.strip())]\n",
    "    return triples\n",
    "\n",
    "\n",
    "def unit_cube_projection(var_matrix):\n",
    "    unit_cube_projection = tf.minimum(1., tf.maximum(var_matrix, 0.))\n",
    "    return tf.assign(var_matrix, unit_cube_projection)\n",
    "\n",
    "\n",
    "def make_batches(size, batch_size):\n",
    "    nb_batch = int(np.ceil(size / float(batch_size)))\n",
    "    res = [(i * batch_size, min(size, (i + 1) * batch_size)) for i in range(0, nb_batch)]\n",
    "    return res\n",
    "\n",
    "class IndexGenerator:\n",
    "    def __init__(self):\n",
    "        self.random_state = np.random.RandomState(0)\n",
    "\n",
    "    def __call__(self, n_samples, candidate_indices):\n",
    "        shuffled_indices = candidate_indices[self.random_state.permutation(len(candidate_indices))]\n",
    "        rand_ints = shuffled_indices[np.arange(n_samples) % len(shuffled_indices)]\n",
    "        return rand_ints\n",
    "\n",
    "class DistMult:\n",
    "    def __init__(self, subject_embeddings=None, object_embeddings=None,\n",
    "                 predicate_embeddings=None,):\n",
    "        self.subject_embeddings, self.object_embeddings = subject_embeddings, object_embeddings\n",
    "        self.predicate_embeddings = predicate_embeddings\n",
    "\n",
    "    def __call__(self):\n",
    "        scores = tf.reduce_sum(self.subject_embeddings *\n",
    "                               self.predicate_embeddings *\n",
    "                               self.object_embeddings, axis=1)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entity_embedding_size = 150\n",
    "predicate_embedding_size = 150\n",
    "\n",
    "seed = 0\n",
    "margin = 5\n",
    "\n",
    "nb_epochs = 1000\n",
    "\n",
    "nb_discriminator_epochs = 1\n",
    "nb_adversary_epochs = 10\n",
    "\n",
    "nb_batches = 10\n",
    "\n",
    "violation_loss_weight = 0.1\n",
    "adversary_batch_size = 1\n",
    "\n",
    "np.random.seed(seed)\n",
    "random_state = np.random.RandomState(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "dataset_name = 'fb122'\n",
    "\n",
    "#train_triples = read_triples('{}/{}.train.tsv'.format(dataset_name, dataset_name))\n",
    "train_triples = read_triples('{}/{}.valid.tsv'.format(dataset_name, dataset_name))\n",
    "\n",
    "valid_triples = read_triples('{}/{}.valid.tsv'.format(dataset_name, dataset_name))\n",
    "test_triples = read_triples('{}/{}.test.tsv'.format(dataset_name, dataset_name))\n",
    "\n",
    "from parse import parse_clause\n",
    "with open('{}/{}-clauses.pl'.format(dataset_name, dataset_name), 'rt') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "clauses = [parse_clause(line.strip()) for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_triples = train_triples + valid_triples + test_triples\n",
    "\n",
    "entity_set = {s for (s, p, o) in all_triples} | {o for (s, p, o) in all_triples}\n",
    "predicate_set = {p for (s, p, o) in all_triples}\n",
    "\n",
    "nb_entities, nb_predicates = len(entity_set), len(predicate_set)\n",
    "nb_examples = len(train_triples)\n",
    "\n",
    "entity_to_idx = {entity: idx for idx, entity in enumerate(sorted(entity_set))}\n",
    "predicate_to_idx = {predicate: idx for idx, predicate in enumerate(sorted(predicate_set))}\n",
    "\n",
    "entity_embedding_layer = tf.get_variable('entities', shape=[nb_entities, entity_embedding_size],\n",
    "                                         initializer=tf.contrib.layers.xavier_initializer())\n",
    "predicate_embedding_layer = tf.get_variable('predicates', shape=[nb_predicates, predicate_embedding_size],\n",
    "                                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "subject_inputs = tf.placeholder(tf.int32, shape=[None])\n",
    "predicate_inputs = tf.placeholder(tf.int32, shape=[None])\n",
    "object_inputs = tf.placeholder(tf.int32, shape=[None])\n",
    "\n",
    "target_inputs = tf.placeholder(tf.float32, shape=[None])\n",
    "\n",
    "subject_embeddings = tf.nn.embedding_lookup(entity_embedding_layer, subject_inputs)\n",
    "predicate_embeddings = tf.nn.embedding_lookup(predicate_embedding_layer, predicate_inputs)\n",
    "object_embeddings = tf.nn.embedding_lookup(entity_embedding_layer, object_inputs)\n",
    "\n",
    "model_parameters = {\n",
    "    'subject_embeddings': subject_embeddings,\n",
    "    'predicate_embeddings': predicate_embeddings,\n",
    "    'object_embeddings': object_embeddings\n",
    "}\n",
    "\n",
    "model_class = DistMult\n",
    "model = model_class(**model_parameters)\n",
    "\n",
    "scores = model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Adversary:\n",
    "    \"\"\"\n",
    "    Utility class for, given a set of clauses, computing the symbolic violation loss.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, clauses, predicate_to_index,\n",
    "                 entity_embedding_layer, predicate_embedding_layer,\n",
    "                 model_class, model_parameters, loss_margin=0.0, batch_size=1):\n",
    "\n",
    "        self.clauses, self.predicate_to_index = clauses, predicate_to_index\n",
    "        self.entity_embedding_layer = entity_embedding_layer\n",
    "        self.predicate_embedding_layer = predicate_embedding_layer\n",
    "\n",
    "        self.entity_embedding_size = self.entity_embedding_layer.get_shape()[-1].value\n",
    "\n",
    "        self.model_class, self.model_parameters = model_class, model_parameters\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        def _violation_losses(body_scores, head_scores, margin):\n",
    "            _losses = tf.nn.relu(margin - head_scores + body_scores)\n",
    "            return tf.reduce_max(_losses)\n",
    "\n",
    "        self.loss_function = lambda body_scores, head_scores:\\\n",
    "            _violation_losses(body_scores, head_scores, margin=loss_margin)\n",
    "\n",
    "        # Symbolic functions computing the continuous loss\n",
    "        self.loss = tf.constant(.0)\n",
    "\n",
    "        # Trainable parameters of the adversarial model\n",
    "        self.parameters = []\n",
    "\n",
    "        # Mapping {clause:v2l} where \"clause\" is a clause, and v2l is a {var_name:layer} mapping\n",
    "        self.clause_to_variable_name_to_layer = dict()\n",
    "        self.clause_to_loss = dict()\n",
    "\n",
    "        for clause_idx, clause in enumerate(clauses):\n",
    "            clause_loss, clause_parameters, variable_name_to_layer =\\\n",
    "                self._parse_clause('clause_{}'.format(clause_idx), clause)\n",
    "\n",
    "            self.clause_to_variable_name_to_layer[clause] = variable_name_to_layer\n",
    "            self.clause_to_loss[clause] = clause_loss\n",
    "\n",
    "            self.loss += clause_loss\n",
    "            self.parameters += clause_parameters\n",
    "\n",
    "    def _parse_atom(self, atom, variable_name_to_layer):\n",
    "        \"\"\"\n",
    "        Given an atom in the form p(X, Y), where X and Y are associated to two distinct [1, k] embedding layers,\n",
    "        return the symbolic score of the atom.\n",
    "        \"\"\"\n",
    "        predicate_idx = self.predicate_to_index[atom.predicate.name]\n",
    "        \n",
    "        # [batch_size x 1 x embedding_size] tensor\n",
    "        predicate_embeddings = tf.nn.embedding_lookup(self.predicate_embedding_layer, [[predicate_idx]] * self.batch_size)\n",
    "        arg1_name, arg2_name = atom.arguments[0].name, atom.arguments[1].name\n",
    "\n",
    "        # [batch_size x embedding_size] variables\n",
    "        arg1_layer, arg2_layer = variable_name_to_layer[arg1_name], variable_name_to_layer[arg2_name]\n",
    "\n",
    "        subject_embeddings = variable_name_to_layer[arg1_name]\n",
    "        object_embeddings = variable_name_to_layer[arg2_name]\n",
    "\n",
    "        model_parameters = self.model_parameters\n",
    "        \n",
    "        model_parameters['subject_embeddings'] = subject_embeddings\n",
    "        model_parameters['object_embeddings'] = object_embeddings\n",
    "        \n",
    "        model_parameters['predicate_embeddings'] = predicate_embeddings\n",
    "\n",
    "        scoring_model = self.model_class(**model_parameters)\n",
    "        atom_score = scoring_model()\n",
    "\n",
    "        return atom_score\n",
    "\n",
    "    def _parse_conjunction(self, atoms, variable_name_to_layer):\n",
    "        \"\"\"\n",
    "        Given a conjunction of atoms in the form p(X0, X1), q(X2, X3), r(X4, X5), return its symbolic score.\n",
    "        \"\"\"\n",
    "        conjunction_score = None\n",
    "        for atom in atoms:\n",
    "            atom_score = self._parse_atom(atom, variable_name_to_layer=variable_name_to_layer)\n",
    "            conjunction_score = atom_score if conjunction_score is None else tf.minimum(conjunction_score, atom_score)\n",
    "        return conjunction_score\n",
    "\n",
    "    def _parse_clause(self, name, clause):\n",
    "        \"\"\"\n",
    "        Given a clause in the form p(X0, X1) :- q(X2, X3), r(X4, X5), return its symbolic score.\n",
    "        \"\"\"\n",
    "        head, body = clause.head, clause.body\n",
    "\n",
    "        # Enumerate all variables\n",
    "        variable_names = {argument.name for argument in head.arguments}\n",
    "        for body_atom in body:\n",
    "            variable_names |= {argument.name for argument in body_atom.arguments}\n",
    "\n",
    "        # Instantiate a new layer for each variable\n",
    "        variable_name_to_layer = dict()\n",
    "        for variable_name in sorted(variable_names):\n",
    "            # [batch_size, embedding_size] variable\n",
    "            variable_layer = tf.get_variable('{}_{}_violator'.format(name, variable_name),\n",
    "                                             shape=[self.batch_size, self.entity_embedding_size],\n",
    "                                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "            variable_name_to_layer[variable_name] = variable_layer\n",
    "\n",
    "        head_score = self._parse_atom(head, variable_name_to_layer=variable_name_to_layer)\n",
    "        body_score = self._parse_conjunction(body, variable_name_to_layer=variable_name_to_layer)\n",
    "\n",
    "        parameters = [variable_name_to_layer[variable_name] for variable_name in sorted(variable_names)]\n",
    "        loss = self.loss_function(body_score, head_score)\n",
    "        return loss, parameters, variable_name_to_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adversary = Adversary(clauses=clauses, predicate_to_index=predicate_to_idx,\n",
    "                      entity_embedding_layer=entity_embedding_layer,\n",
    "                      predicate_embedding_layer=predicate_embedding_layer,\n",
    "                      model_class=model_class, model_parameters=model_parameters,\n",
    "                      batch_size=adversary_batch_size)\n",
    "\n",
    "adversary_init_op = tf.variables_initializer(var_list=adversary.parameters, name='init_adversary')\n",
    "violation_loss = adversary.loss\n",
    "\n",
    "ADVERSARIAL_OPTIMIZER_SCOPE_NAME = 'adversary/optimizer'\n",
    "with tf.variable_scope(ADVERSARIAL_OPTIMIZER_SCOPE_NAME):\n",
    "    adversarial_optimizer = tf.train.AdagradOptimizer(learning_rate=0.1)\n",
    "    adversarial_training_step = adversarial_optimizer.minimize(- violation_loss, var_list=adversary.parameters)\n",
    "\n",
    "adversary_optimizer_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=ADVERSARIAL_OPTIMIZER_SCOPE_NAME)\n",
    "adversary_optimizer_vars_init_op = tf.variables_initializer(adversary_optimizer_vars)\n",
    "\n",
    "adversary_projections = [unit_cube_projection(emb) for emb in adversary.parameters]\n",
    "\n",
    "\n",
    "hinge_losses = tf.nn.relu(margin - scores * (2 * target_inputs - 1))\n",
    "\n",
    "loss = tf.reduce_sum(hinge_losses) + violation_loss_weight * violation_loss\n",
    "\n",
    "optimizer = tf.train.AdagradOptimizer(learning_rate=0.1)\n",
    "training_step = optimizer.minimize(loss, var_list=[entity_embedding_layer, predicate_embedding_layer])\n",
    "\n",
    "projection_step = unit_cube_projection(entity_embedding_layer)\n",
    "\n",
    "\n",
    "import math\n",
    "batch_size = math.ceil(nb_examples / nb_batches)\n",
    "batches = make_batches(nb_examples, batch_size)\n",
    "\n",
    "nb_versions = 3\n",
    "\n",
    "Xs = np.array([entity_to_idx[s] for (s, p, o) in train_triples], dtype=np.int32)\n",
    "Xp = np.array([predicate_to_idx[p] for (s, p, o) in train_triples], dtype=np.int32)\n",
    "Xo = np.array([entity_to_idx[o] for (s, p, o) in train_triples], dtype=np.int32)\n",
    "\n",
    "index_gen = IndexGenerator()\n",
    "\n",
    "init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f3c5da9625f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0madversarial_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0massignment_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mground_init_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0memb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0madversary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massignment_ops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-f3c5da9625f9>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0madversarial_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0massignment_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mground_init_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0memb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0madversary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massignment_ops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-f3c5da9625f9>\u001b[0m in \u001b[0;36mground_init_op\u001b[0;34m(adversarial_embeddings)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mground_init_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madversarial_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             rnd_entity_indices = entity_indices[\n\u001b[0;32m---> 55\u001b[0;31m                  random_state.randint(low=0, high=len(entity_indices), size=adversary_batch_size)]\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0mentity_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_lookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity_embedding_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnd_entity_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0madversarial_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "def stats(values):\n",
    "    return '{0:.4f} Â± {1:.4f}'.format(round(np.mean(values), 4), round(np.std(values), 4))\n",
    "\n",
    "session = tf.Session()\n",
    "session.run(init_op)\n",
    "\n",
    "for epoch in range(1, nb_epochs + 1):\n",
    "    \n",
    "    \n",
    "    for discriminator_epoch in range(1, nb_discriminator_epochs + 1):\n",
    "        order = random_state.permutation(nb_examples)\n",
    "        Xs_shuf, Xp_shuf, Xo_shuf = Xs[order], Xp[order], Xo[order]\n",
    "\n",
    "        loss_values = []\n",
    "\n",
    "        for batch_no, (batch_start, batch_end) in enumerate(batches):\n",
    "            curr_batch_size = batch_end - batch_start\n",
    "\n",
    "            Xs_batch = np.zeros(curr_batch_size * nb_versions, dtype=Xs_shuf.dtype)\n",
    "            Xp_batch = np.zeros(curr_batch_size * nb_versions, dtype=Xp_shuf.dtype)\n",
    "            Xo_batch = np.zeros(curr_batch_size * nb_versions, dtype=Xo_shuf.dtype)\n",
    "\n",
    "            Xs_batch[0::nb_versions] = Xs_shuf[batch_start:batch_end]\n",
    "            Xp_batch[0::nb_versions] = Xp_shuf[batch_start:batch_end]\n",
    "            Xo_batch[0::nb_versions] = Xo_shuf[batch_start:batch_end]\n",
    "\n",
    "            # Xs_batch[1::nb_versions] needs to be corrupted\n",
    "            Xs_batch[1::nb_versions] = index_gen(curr_batch_size, np.arange(nb_entities))\n",
    "            Xp_batch[1::nb_versions] = Xp_shuf[batch_start:batch_end]\n",
    "            Xo_batch[1::nb_versions] = Xo_shuf[batch_start:batch_end]\n",
    "\n",
    "            # Xo_batch[2::nb_versions] needs to be corrupted\n",
    "            Xs_batch[2::nb_versions] = Xs_shuf[batch_start:batch_end]\n",
    "            Xp_batch[2::nb_versions] = Xp_shuf[batch_start:batch_end]\n",
    "            Xo_batch[2::nb_versions] = index_gen(curr_batch_size, np.arange(nb_entities))\n",
    "\n",
    "            feed_dict = {\n",
    "                subject_inputs: Xs_batch, predicate_inputs: Xp_batch, object_inputs: Xo_batch,\n",
    "                target_inputs: np.array([1.0, 0.0, 0.0] * curr_batch_size)\n",
    "            }\n",
    "\n",
    "            _, loss_value = session.run([training_step, loss], feed_dict=feed_dict)\n",
    "            session.run(projection_step)\n",
    "\n",
    "            loss_values += [loss_value / (Xp_batch.shape[0] / nb_versions)]\n",
    "\n",
    "\n",
    "    session.run([adversary_init_op, adversary_optimizer_vars_init_op])\n",
    "    entity_indices = sorted(entity_to_idx.values())\n",
    "        \n",
    "    def ground_init_op(adversarial_embeddings):\n",
    "        rnd_entity_indices = entity_indices[\n",
    "            random_state.randint(low=0, high=len(entity_indices), size=adversary_batch_size)]\n",
    "        entity_embeddings = tf.nn.embedding_lookup(entity_embedding_layer, rnd_entity_indices)\n",
    "        return adversarial_embeddings.assign(entity_embeddings)\n",
    "        \n",
    "    assignment_ops = [ground_init_op(emb) for emb in adversary.parameters]\n",
    "    session.run(assignment_ops)\n",
    "            \n",
    "        \n",
    "    for adversary_epoch in range(1, nb_adversary_epochs + 1):\n",
    "        pass\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        logger.info('Epoch {0}\\tLoss value: {1}'.format(epoch, stats(loss_values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eval_name, eval_triples in [('valid', valid_triples), ('test', test_triples)]:\n",
    "\n",
    "    ranks_subj, ranks_obj = [], []\n",
    "    filtered_ranks_subj, filtered_ranks_obj = [], []\n",
    "\n",
    "    for _i, (s, p, o) in enumerate(eval_triples):\n",
    "        s_idx, p_idx, o_idx = entity_to_idx[s], predicate_to_idx[p], entity_to_idx[o]\n",
    "\n",
    "        Xs = np.full(shape=(nb_entities,), fill_value=s_idx, dtype=np.int32)\n",
    "        Xp = np.full(shape=(nb_entities,), fill_value=p_idx, dtype=np.int32)\n",
    "        Xo = np.full(shape=(nb_entities,), fill_value=o_idx, dtype=np.int32)\n",
    "\n",
    "        feed_dict_corrupt_subj = {subject_inputs: np.arange(nb_entities), predicate_inputs: Xp, object_inputs: Xo}\n",
    "        feed_dict_corrupt_obj = {subject_inputs: Xs, predicate_inputs: Xp, object_inputs: np.arange(nb_entities)}\n",
    "\n",
    "        # scores of (1, p, o), (2, p, o), .., (N, p, o)\n",
    "        scores_subj = session.run(scores, feed_dict=feed_dict_corrupt_subj)\n",
    "\n",
    "        # scores of (s, p, 1), (s, p, 2), .., (s, p, N)\n",
    "        scores_obj = session.run(scores, feed_dict=feed_dict_corrupt_obj)\n",
    "\n",
    "        ranks_subj += [1 + np.sum(scores_subj > scores_subj[s_idx])]\n",
    "        ranks_obj += [1 + np.sum(scores_obj > scores_obj[o_idx])]\n",
    "\n",
    "        filtered_scores_subj = scores_subj.copy()\n",
    "        filtered_scores_obj = scores_obj.copy()\n",
    "\n",
    "        rm_idx_s = [entity_to_idx[fs] for (fs, fp, fo) in all_triples if fs != s and fp == p and fo == o]\n",
    "        rm_idx_o = [entity_to_idx[fo] for (fs, fp, fo) in all_triples if fs == s and fp == p and fo != o]\n",
    "\n",
    "        filtered_scores_subj[rm_idx_s] = - np.inf\n",
    "        filtered_scores_obj[rm_idx_o] = - np.inf\n",
    "\n",
    "        filtered_ranks_subj += [1 + np.sum(filtered_scores_subj > filtered_scores_subj[s_idx])]\n",
    "        filtered_ranks_obj += [1 + np.sum(filtered_scores_obj > filtered_scores_obj[o_idx])]\n",
    "\n",
    "        if _i % 1000 == 0:\n",
    "            logger.info('{}/{} ..'.format(_i, len(eval_triples)))\n",
    "        \n",
    "        \n",
    "    ranks = ranks_subj + ranks_obj\n",
    "    filtered_ranks = filtered_ranks_subj + filtered_ranks_obj\n",
    "\n",
    "    for setting_name, setting_ranks in [('Raw', ranks), ('Filtered', filtered_ranks)]:\n",
    "        mean_rank = np.mean(setting_ranks)\n",
    "        logger.info('[{}] {} Mean Rank: {}'.format(eval_name, setting_name, mean_rank))\n",
    "        for k in [1, 3, 5, 10]:\n",
    "            hits_at_k = np.mean(np.asarray(setting_ranks) <= k) * 100\n",
    "            logger.info('[{}] {} Hits@{}: {}'.format(eval_name, setting_name, k, hits_at_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
